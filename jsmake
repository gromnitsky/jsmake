#!/usr/bin/env node

let util = require('util')
let path = require('path')

let inspect = v => util.inspect(v, {depth:null})

class FToken {
    constructor(tag, val, src, line, col, row) {
	this.tag = tag
	this.val = val
	this.src = src
	this.line = line
	this.col = col
	this.row = row
    }

    pos() { return `${this.src}:${this.line}:${this.col}:${this.row}` }
    inspect() { return `${this.tag}:${this.val}:${this.pos()}`}
}

class FirstTokenizer {
    // input -- a string to tokenize
    // src   -- a file name from which the input came from
    constructor(input, src = '-') {
	this.input = input.split("\n")

	this.tokens = [
	    { type: 'comment', re: '#.*$' },
	    { type: 'recipe',  re: '\t.*$' },
	    { type: 'id', re: '[^=:]+' },
	    { type: 'op',  re: '[=:]'},
	    { type: 'rvalue',  re: '.*$' },
	]
	this.line = 0
	this.src = src
    }

    tokenize() {
	let r = []
	let line_prev = ''
	for (let line of this.input) {
	    this.line++
	    let column = 0
	    if (line.match(/^\s*$/)) continue

	    if (line.match(/\\$/)) { // join lines that end with `\`
		line_prev += line.replace(/\\$/, '')
		continue
	    } else {
		line = line_prev + line
		line_prev = ''
	    }

	    while (line.length) {
		for (let token of this.tokens) {
		    if (!line.length) break

		    let m = line.match(`^${token.re}`)
		    if (!m) continue
		    let skip = m.index + m[0].length
		    line = line.slice(skip)

		    r.push(new FToken(token.type, m[0].trim(), this.src,
				      this.line, column, column + skip - 1))
		    column += skip
		}
	    }
	}
	return r
    }
}

class Parser {
    constructor(tokens) {
	this.tokens = tokens
	this.vars = {}
	this.rules = []
    }

    parse() {
	let skip = -1
	for (let idx = 0; idx < this.tokens.length; ++idx) {
	    if (skip > idx) continue

	    let cur = this.tokens[idx]
	    let next = this.tokens[idx+1]

	    if (cur.tag === 'id' && next && next.tag === 'op') {
		skip = next.val === '=' ? this.variable(idx) : this.rule(idx)
	    } else if (cur.tag === 'comment') {
		// TODO: annotate the last rule
	    } else {
		throw new Error(`${cur.pos()}: unexpected ${cur.tag}: ${cur.val}`)
	    }
	}
    }

    rule(idx) {
	let r = {
	    target: this.tokens[idx].val,
	}
	idx += 2
	if (this.tokens[idx] && this.tokens[idx].tag === 'rvalue') {
	    r.deps = this.tokens[idx].val
	    idx++
	}

	let ri = idx
	let recipes = []
	while (this.tokens[ri] && this.tokens[ri].tag === 'recipe') {
	    recipes.push(this.tokens[ri].val)
	    ri++
	}

	if (recipes.length) r.recipes = recipes
	this.rules.push(r)
	return ri
    }

    variable(idx) {
	let name = this.tokens[idx].val
	idx += 2
	if (this.tokens[idx] && this.tokens[idx].tag !== 'rvalue') {
	    this.vars[name] = ''
	    return idx
	}
	this.vars[name] = this.tokens[idx] ? this.tokens[idx].val : ''
	return idx+1
    }
}

let Functions = {
    dir: (names) => {
	return names.split(/\s+/).map( val => {
	    let p = path.dirname(val)
	    return p === '/' ? p : p + '/'
	}).join(' ')
    },
    subst: (from, to, text) => {
	return text.replace(new RegExp(from, 'g'), to)
    }
}

class Expander {
    constructor(parser, functions) {
	this.parser = parser
	this.functions = functions || {}

	this.input = null
	this.tok = null
	this.vars = {}
	this.rules = []

	this.log = console.error
    }

    advance() {
	if (!this.input.length) return this.tok = null // end of input

	let lexems = [
		/^[^$()]+/,
		/^./
	]
	for (let val of lexems) {
	    let m = this.input.match(val)
	    if (m) {
		this.tok = m[0]
		this.input = this.input.slice(m[0].length)
		return this.tok
	    }
	}
	throw new Error(`lexer failure: ${this.input}`)
    }

    parse(str) {
	this.input = str
	let r = []

	this.advance()
	while (this.tok !== null) {
	    if (this.tok === '$') r.push(this.func())
	    else {
		r.push({ val: this.tok })
		this.advance()
	    }
	}
	return r
    }

    // updates this.parser.{vars,rules}
    expand() {
	let e_vars = {}
	for (let varname in this.parser.vars) {
	    let val = this.parser.vars[varname]

	    let e_varname = this._expand(this.parse(varname))
	    let e_val = this._expand(this.parse(val), e_varname)
	    e_vars[e_varname] = e_val
	}
	this.parser.vars = e_vars

	let e_rules = []
	for (let rule of this.parser.rules) {
	    let r = { target: this._expand(this.parse(rule.target)) }
	    if (rule.deps) r.deps = this._expand(this.parse(rule.deps))
	    if (rule.recipes) {
		// rules are being expanded during the invocation, not here
		r.recipes = rule.recipes
	    }
	    e_rules.push(r)
	}
	this.parser.rules = e_rules
    }

    // crazy recursive
    _expand(arr, wantedby) {
	return arr.map( item => {
	    // a non-expandable portion
	    if (item.type !== '$') return (item.val || item)

	    // turtles all the way down
	    let r = this._expand(item.val, wantedby)
	    let defined = this.parser.vars[r]
	    // the value of the variable we've found can be expanded too
	    if (defined) {
		if (wantedby && wantedby === r)
		    throw new Error(`var '${r}' references itself`)
		return this._expand(this.parse(defined), wantedby)
	    }

	    // check r in the list of known functions; TODO: refactor out
	    let fname = r.split(' ')[0]
	    if (fname in this.functions) {
		// remove the leading spaces only for the 1st arg
		// before passing them all to the function
		let fargs = r.split(' ').slice(1).join(' ').
		    trimLeft().split(',')
		return this.functions[fname].apply(this, fargs)
	    }

	    return '?'		// FIXME
	}).join('')
    }

    func() {
	let result = (val) => ({ type: '$', val})
	let r = []

	this.eat('$')
	this.eat('(')

	if (this.tok === ')') {
	    this.advance()
	    this.log('an attempt to expand empty space')
	    return result([])
	}

	while (this.tok !== ')') {
	    if (this.tok === null) throw new Error(`expected ')'`)

	    if (this.tok === '$') r.push(this.func())
	    else {
		r.push(this.tok)
		this.advance()
	    }
	}

	this.eat(')')
	return result(r)
    }

    eat(expected_token) {
	if (this.tok !== expected_token)
	    throw new Error(`saw ${this.tok}, expected ${expected_token}`)
	this.advance()
    }
}

exports.FirstTokenizer = FirstTokenizer
exports.Parser = Parser
exports.Expander = Expander
exports.Functions = Functions

// Main
if (process.argv[1] === __filename) {
    let concat = require('concat-stream')

    let concat_stream = concat( input => {
	let tokens = new FirstTokenizer(input.toString()).tokenize()
	console.log(tokens)

	let parser = new Parser(tokens)
	parser.parse()
	console.log(parser.vars)
	console.log(parser.rules)

	console.log('')
	let expander = new Expander(parser, Functions)
	expander.expand()
	console.log(parser.vars)
	console.log(parser.rules)
    })

    process.stdin.pipe(concat_stream)
}
